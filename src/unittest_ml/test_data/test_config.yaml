general:
  client_number: 10
  training_rounds: 10

train_config:
  epochs: 5
  batch_size: 64
  shuffle: True

optimizer:
  type: SGD           # Options: SGD, Adam, Adagrad, RMSprop, etc.
  lr: 0.01            # Learning rate
  weight_decay: 0.0001  # Weight decay (set to None if not used)
  momentum: 0       # Momentum (for SGD and RMSprop, set to None if not used)
  nesterov: False     # Whether to use Nesterov momentum (only applicable for SGD; set to None if not used)
  betas: [0.9, 0.999] # Betas parameter for Adam (set to None if not used)
  amsgrad: False      # Whether to use AMSGrad (only applicable for Adam; set to None if not used)
  eps: 1e-8           # Epsilon parameter for Adam (set to None if not used)
  alpha: None         # Alpha parameter for RMSprop (set to None if not used)
  centered: None      # Centered parameter for RMSprop (set to None if not used)

loss_func:
  type: CrossEntropyLoss  # Options: CrossEntropyLoss, MSELoss, etc.
  reduction: mean         # Options: mean, sum, none
  weight: None            # Optional: Define class weights for imbalanced datasets

client_selection:
  method: random # can be fedgra....
  client_selection_round: 1
  number_client_selected: 2

aggregation:
  method: fedavg